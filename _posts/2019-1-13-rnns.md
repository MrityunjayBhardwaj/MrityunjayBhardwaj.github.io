---
layout: single
title : Understanding Recurrent Neural Nets
tags  : [ML,DL,AI,RNN,neural-networks]
title-seprator: "|"
categories: blog MachineLearning
permalink: /:categories/:title.html
mathjax: true
author-bio: false
author_profile: false
header:
    teaser: /assets/imgs/posts_imgs/rnn_with_math/RNNunfolded.png
---

Recurrent Neural Nets are one of the most crucial network architecture to understand in the field of deep learning and for good reason. today, when it comes to modelling sequential data our goto architecture is RNNs,weather it's predicting what are you going to type next or to create your own Siri,Alexa or even do some time series prediction like predicting stock prices etc…. in short… its important to understand it!

Now that you are convinced! we can move forward into really understanding it… don't worry its not that unfamiliar…. its just a simple Neural-net with some dynamical system aspects that are incorporated into its architecture and with a twist in its backpropogation algorithm... so lets get into it!

As discussed before, RNNs are used to model sequential data which are generated from dynamical systems like our speech, we can think of dynamical system like a function which depends on the output of the previous time step.. i.e, when we utter a sentence our choice of current word is bounded by the choice of our previous word and so on.. for example, we can't just say "Today harry a beautiful day" instead of "Today is a beautiful day"… thats why its a squential problem..

so this is a mathematical intrepretation of of what we have just discussed


$$ S_t = f(S_{t-1}) $$

and if we were to represent it in computational graph it looks something like this:

<img src="{{site.url}}{{site.baseurl}}/assets/imgs/posts_imgs/rnn_with_math/RNNunfolded.png">


Some Notations:-

$$ \text{S = State or hidden units}$$
$$ \text{V = Weight Matrix b/w hidden unit and the output} $$
$$ \text{W = Weight Matrix b/w S of current time step to S of time step}$$
$$ \text{U =  Weight Matrix b/w inputs and hidden units}$$


now that you understood what it means to be a dynamical system we can unfold the computation graph and see what is happening...

as you can see in the diagram above that we are essentially inputting our sentence word by word sequentially at each time step to the hidden units and then to the output units, think of it like a Vertical simple neuralnet...the only twist in here,is that we are connecting sequentially!

as you can see in the diagram aboce that we are essentially inputting our sentence to a hidden unit and then connecting it to the output unit you can think of it like a vertical neural net but here, we are doing this for every word in our input sentence and connecting it sequentially as you can see above and just like simple neural nets we have weights at each connections

the output node outputs the probability of occurence of the next word in the squence uptill now... we can use the output at time step t and predict the next word...

but what if we got it wrong?? first of all we need a measure to calculate the correctness of our prediction, here comes our Loss function, our loss function is just a accumulation of all the individual loss at each time step... i.e, $$ L_t = l(o_t - y_t) $$ where y_t is the true output.

Now that we know how to measure our accuracy we can now focus on how to minimize it. in other words, we need to come up with Weights that minimize our Loss function.

the way we are going to be doing that is by calculate the  derivative of Loss function w.r.t. these weights which will tells us in which direction we should move in order to minimize our Loss function and update our weights accordingly.

a.k.a we need:-

$$ { {\partial L}\over{\partial V} },
 { {\partial L}\over{\partial W} }, 
 { {\partial L}\over{\partial U} } $$

but in order to calculate we first need to calculate these derivatives:-
--------------------------------------------

in here we are inputting our sentence word-by-word squentially and each time step represent each word essentially.

$$ a_t = b + WS_{t-1} + Ux_t $$
$$ S_t = \sigma(a_t) $$
$$ o_t = c + Vs_t $$
$$ softmax(o_t) $$


our final y_hat is the summation of all the output probability

and our loss function is :=

$$ L = \sum_t(\hat o_t - o_t)^2 $$

for language modelling usually its cross entropy

just like in NN our goal is to find the weights which minimize our loss function so in order to do that we are going to be calculating the derivative and update our weights accordingly

$$ { {\partial L}\over{\partial o_t}} = { {\partial L}\over{\partial L_t}} * { {\partial L_t}\over{\partial o_t} } $$

if I chage the output of this time step it doenst change the output in other time step because they are not connected.
for second term : we can calculate the derivative of loss w.r.t output at time step (t) by just... well doing the derivative accoring to what our loss function is.

so our final expression is : 

$$ { {\partial L} \over {\partial L_t} } = 1 $$
$$ { {\partial L_t} \over {\partial o_t} }  = Something... $$ 
$$ { {\partial L} \over{\partial o_t} } = 1 * Something $$


$$ {}$$

this is not an easy fleet... beacuse if you cange S_t its going to change S_t+1 and s_t+2 and so on... untill the end of the time

$$ { {\partial L} \over{\partial s_t}} = { {\partial L} \over {\partial o_t} } * { {\partial o_t} \over {\partial s_t} } $$

here, we have calculate the first part... but what about the second one??

as we can see, o_t = c+ V*s_t so derivative w.r.t. s_t is just V

so our final derivative will be:

....................

this is fine but as we have discussed before, it is also going to affect the next time step which inturen change our final loss so our value is going to be

$$ { {\partial L} \over{\partial s_t}} = ({ {\partial L} \over {\partial o_t} } * { {\partial o_t} \over {\partial s_t} }) + ({ {\partial L}\over{\partial s_{t+1}} } * { {\partial s_{t+1}}\over {\partial s_t} }) $$

here we have a problem basically we want to calcaulate the derivative w.r.t current time but as u can see in the expression about we also need to calculate the derivative w.r.t the next time step and so on... untill the end of time...(litrally)

that is why we first need to calculate the derivative w.r.t last time-step or layer and then **backpropagate through time** to get to the current time step (t) that is the reason why we called it backpropagation through(BPTT) time algorithm



so our final expression will be

for convenience lets write 
$$ \kappa = { {\partial L} \over{\partial s} } $$
$$ \kappa_t = ({ {\partial L} \over {\partial o_t} } * V) + (\kappa_{t+1} * (W*{ {\partial \sigma_{t}}\over {\partial s_t} }) ) $$

Now we have all the ingredients we need in order to calculate the derivatives of our weights

$${ {\partial L} \over{\partial V}} = { {\partial L} \over{\partial o_t} } * { {\partial o_t} \over{\partial V} }$$



$${ {\partial L} \over{\partial W}} = { {\partial L} \over{\partial s_t} } * { {\partial s_t} \over{\partial W} }$$


$${ {\partial L} \over{\partial U}} = { {\partial L} \over{\partial s_t} } * { {\partial s_t} \over{\partial U} }$$


Now that we have the derivatives of all the weights we can finally compute our weights

$$ V_{\text{new}} = V_{\text{old}} + \alpha{ {\partial L} \over{\partial V} } $$

$$ W_{\text{new}} = W_{\text{old}} + \alpha{ {\partial L} \over{\partial W} } $$

$$ U_{\text{new}} = U_{\text{old}} + \alpha{ {\partial L} \over{\partial U} } $$

Congratulations,now that you know how Recurrent Neural Network works mathematically as well as intuitively...we can worry about how to implement it in practice... if you as anyone in the industry they all say RNNs doesnt work as it is... in paper it is really great and this is how it works internall but in practice the implementation is really tough we need to do some modification in order to make it work and that we will do in the next part... but for the time being here are some links which implements the RNNs :-
<a href="#">  </>
